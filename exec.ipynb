{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from src import env_builder\n",
    "\n",
    "# Carrega as variáveis de ambiente\n",
    "_ = load_dotenv()\n",
    "\n",
    "# Obtém o SUBSCRIPTION_ID do ambiente\n",
    "SUBSCRIPTION_ID = os.environ.get('SUBSCRIPTION_ID')\n",
    "\n",
    "# Cria um nome para o grupo de recursos\n",
    "resource_group_name = 'rg-dev-ml-eastus-001'\n",
    "# Seleciona uma localização\n",
    "location = 'eastus'\n",
    "# Cria o recurso\n",
    "env_builder.get_or_create_rg(\n",
    "    resource_group_name=resource_group_name,\n",
    "    location=location,\n",
    "    subscription_id=SUBSCRIPTION_ID\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rafae\\Documents\\Projects\\end-to-end-ml-post\\src\\env_builder.py:68: UserWarning: Adicionando acesso anônimo ao blob.\n",
      "  warnings.warn('Adicionando acesso anônimo ao blob.')\n"
     ]
    }
   ],
   "source": [
    "# Cria um nome para a conta de armazenamento\n",
    "storage_account_name = 'datadevmleastus001'\n",
    "# Cria o recurso\n",
    "rg_deployment_result = env_builder.create_storage_account(\n",
    "    storage_account_name=storage_account_name,\n",
    "    subscription_id=SUBSCRIPTION_ID,\n",
    "    resource_group_name=resource_group_name,\n",
    "    location=location\n",
    ")\n",
    "# Obtém informações de saída sobre a conta de armazenamento\n",
    "storage_account_info = rg_deployment_result.properties.as_dict()['outputs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# depois de criar a storage account e coletar a connection string\n",
    "os.environ['CONNECTION_STRING_STORAGE_ACCOUNT'] = storage_account_info['storageAccountConnectionString']['value']\n",
    "conn_string_storage_account = os.environ.get('CONNECTION_STRING_STORAGE_ACCOUNT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cria um cliente Blob usando a string de conexão\n",
    "blob_client = env_builder.get_blob_service_client_connection_string(\n",
    "    conn_string_storage_account=conn_string_storage_account\n",
    ")\n",
    "\n",
    "# Nome do contêiner\n",
    "container_name = 'data4ml'\n",
    "\n",
    "# Cria o contêiner e/ou recupera o cliente do contêiner\n",
    "container_client = env_builder.create_blob_container(blob_service_client=blob_client, container_name=container_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# carregar os dados como um DataFrame pandas\n",
    "df = pd.read_csv('heart.csv', index_col=0)\n",
    "# utilizamos o modelo de split do scikit-learn para separar 10% dos dados como dataset de test\n",
    "xtrain, xtest, _, _ = train_test_split(df, df['output'], test_size=0.1)\n",
    "# salvamos os datasets separados para podermos testar a inferencia com o `heart.test.csv`\n",
    "xtrain.to_csv('heart.csv', index=False)\n",
    "xtest.to_csv('heart.test.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Caminho para o arquivo do conjunto de dados\n",
    "filepath = 'heart.csv'\n",
    "# Faz o upload do arquivo usando o cliente do contêiner\n",
    "env_builder.upload_blob_file(\n",
    "    filepath=filepath, blobname=filepath,\n",
    "    container_client=container_client\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nome do workspace a ser criado\n",
    "workspace = 'dev-heart-classifier'\n",
    "\n",
    "# Criando o workspace\n",
    "ml_client = env_builder.get_mlclient_and_create_workspace(\n",
    "    subscription_id=SUBSCRIPTION_ID,\n",
    "    resource_group=resource_group_name,\n",
    "    location=location,\n",
    "    workspace=workspace\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Caminho para o arquivo yaml\n",
    "conda_yaml_path='conda.yaml'\n",
    "# Imagem base a ser usada no Environment\n",
    "base_image=\"mcr.microsoft.com/azureml/openmpi4.1.0-ubuntu20.04:latest\"\n",
    "# Descrição do ambiente, pode conter qualquer informação útil.\n",
    "description = ''\n",
    "\n",
    "# Cria o Environment\n",
    "job_env = env_builder.get_or_create_enviroment(\n",
    "    ml_client=ml_client, conda_yaml_path=conda_yaml_path,\n",
    "    base_image=base_image, description=description\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nome do workspace onde o cluster será criado\n",
    "workspace = 'dev-heart-classifier'\n",
    "# Configuração do cluster\n",
    "config = {\n",
    "    'name': \"heartclassifycluster\",  # Nome do cluster\n",
    "    'type': \"amlcompute\",\n",
    "    'size': \"STANDARD_DS3_v2\",  # Instância de computação a ser usada. Isso pode mudar dependendo da carga de trabalho\n",
    "    'location': \"eastus\",  # Localização do recurso\n",
    "    'min_instances': 0,\n",
    "    'max_instances': 1,\n",
    "    'idle_time_before_scale_down': 10  # Tempo ocioso antes de desligar o recurso de computação. Isso ajuda a lançar vários trabalhos sem se preocupar que a VM esteja ligada o tempo todo.\n",
    "}\n",
    "# Cria o cluster e retorna os detalhes da operação\n",
    "cluster_info = env_builder.get_or_create_compute_cluster(\n",
    "    ml_client=ml_client, cluster_config=config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = env_builder.get_datafile_in_blob_for_job_sweep(\n",
    "    containername=container_name, accountname=storage_account_name,\n",
    "    filename=filepath, folder=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import azure.ai.ml.sweep as sweep_types\n",
    "\n",
    "# Comando bash que lança o treinamento\n",
    "command_str = 'python model_train.py --input_data ${{inputs.input_data}} --model_name ${{inputs.model_name}} --test_size ${{inputs.test_size}} --n_estimators ${{inputs.n_estimators}} --max_depth ${{inputs.max_depth}} --criterion ${{inputs.criterion}} --max_features ${{inputs.max_features}}'\n",
    "\n",
    "# Nome do modelo usado para salvar o arquivo pickle\n",
    "model_name = 'rf_model_test'\n",
    "\n",
    "# Informações de input que serão fixadas em todas as execuções + parâmetros padrão para a busca de hiperparâmetros\n",
    "inputs = {\n",
    "    'input_data': input_data,\n",
    "    'model_name': model_name,\n",
    "    'test_size': 0.3,\n",
    "    'n_estimators': 100,\n",
    "    'max_depth': -1,\n",
    "    'criterion': 'gini',\n",
    "    'max_features': 'sqrt',\n",
    "}\n",
    "\n",
    "# Parâmetros que terão seus valores alterados durante a busca\n",
    "sweep_inputs = {\n",
    "    'n_estimators': sweep_types.Choice([10, 50, 100, 125, 150]),\n",
    "    'max_depth': sweep_types.Choice([4, 6, 8, 10, -1]),\n",
    "    'criterion': sweep_types.Choice([\"gini\", \"entropy\"]),\n",
    "    'max_features': sweep_types.Choice([\"sqrt\", \"log2\"])\n",
    "}\n",
    "\n",
    "# Nome do cluster de computação\n",
    "compute = \"heartclassifycluster\"\n",
    "# Nome do ambiente\n",
    "environment='sklearn-env:1'\n",
    "# Nome do experimento\n",
    "experiment_name='test_hyper_sweep'\n",
    "# Lança a busca de hiperparâmetros\n",
    "_ = env_builder.launch_hyperparam_search(\n",
    "    command_str=command_str, inputs=inputs,\n",
    "    sweep_inputs=sweep_inputs, experiment_name=experiment_name,\n",
    "    ml_client=ml_client, compute=compute, environment=environment,\n",
    "    max_total_trials=25, max_concurrent_trials=1,   \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Acesse os jobs do ml_client e obtenha aquele que corresponde ao nome do experimento configurado anteriormente.\n",
    "jobs = ml_client.jobs.list()\n",
    "jobs = list(filter(lambda x: x.display_name == experiment_name, jobs))\n",
    "sweep_job = jobs[0]\n",
    "\n",
    "best_model = env_builder.register_best_model_from_sweep(\n",
    "    ml_client=ml_client, returned_sweep_job=sweep_job,\n",
    "    model_name=model_name, register_name=f'best_{model_name}'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nome a ser usado\n",
    "endpoint_name = 'heart-classify-endpoint'\n",
    "deployment_name = 'heart-classify-deployment'\n",
    "get_model_name = f'best_{model_name}'\n",
    "\n",
    "# Você pode acessar o melhor modelo usando o ml client\n",
    "melhor_modelo = ml_client.models.get(get_model_name, version=1)\n",
    "\n",
    "# Você também pode acessar o ambiente usando o ml client\n",
    "model_environment = ml_client.environments.get(\n",
    "    name=environment.split(':')[0],\n",
    "    version=environment.split(':')[1]\n",
    ")\n",
    "\n",
    "# Criar endpoint\n",
    "_ = env_builder.create_endpoint_specs(ml_client=ml_client, endpoint_name=endpoint_name)\n",
    "_ = env_builder.get_or_create_endpoint_deployment(\n",
    "        ml_client=ml_client, deployment_name=deployment_name,\n",
    "        model=melhor_modelo, env=model_environment,\n",
    "        endpoint_name=endpoint_name, code=\"./src\", filepath=\"score.py\",\n",
    "        instance_type='Standard_DS1_v2'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Adquirindo o token e o URL do endpoint\n",
    "_ = load_dotenv()\n",
    "endpoint = ml_client.online_endpoints.get(name=endpoint_name)\n",
    "endpoint_token = os.environ.get('ENDPOINT_TOKEN')\n",
    "url = endpoint.scoring_uri\n",
    "\n",
    "headers = {\n",
    "    'Content-Type':'application/json',\n",
    "    'Authorization':('Bearer '+ endpoint_token),\n",
    "    'azureml-model-deployment': deployment_name\n",
    "}\n",
    "\n",
    "# Carregamos os dados e os enviamos\n",
    "df = pd.read_csv('heart.csv', index_col=0)\n",
    "# df = pd.read_csv('heart.csv', index_col=0).drop(columns=['Unnamed: 0'])\n",
    "df_input = df.drop(columns=['output'])\n",
    "samples_target = df['output'].values\n",
    "samples_ready_to_send = {\n",
    "    'data': df_input.to_dict(orient='records')\n",
    "}\n",
    "\n",
    "response = requests.post(url, data=str.encode(json.dumps(samples_ready_to_send)), headers=headers)\n",
    "inference_result_request = np.array(response.json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(samples_target, inference_result_request))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env_builder.ml_workspace_deletion(ml_client=ml_client, workspace=workspace)\n",
    "\n",
    "# delete_result = env_builder.get_and_delete_rg(\n",
    "#     resource_group_name=resource_group_name,\n",
    "#     subscription_id=SUBSCRIPTION_ID\n",
    "# )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
